{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MSDialoGPT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MGQAB8Z7zeQ",
        "outputId": "8853082d-a39d-47a5-d610-2280ba759c88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/4e/4f1ede0fd7a36278844a277f8d53c21f88f37f3754abf76a5d6224f76d4a/transformers-3.4.0-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 10.4MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 47.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers==0.9.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a5/78be1a55b2ac8d6a956f0a211d372726e2b1dd2666bb537fea9b03abd62c/tokenizers-0.9.2-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 48.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 44.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=fc20079e32028f2cc8936a8240df231691c7b88bbfa6c450a88692ace7c2e4ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.94 tokenizers-0.9.2 transformers-3.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX_SJmlp7ZW7",
        "outputId": "7aa6372d-86d8-4334-ffc1-ab02afe5557d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "from collections import deque\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "HISTORY_DEPTH = 5\n",
        "history = deque([], HISTORY_DEPTH)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Let's chat for a maximum of 10 lines. Keep the history short.\n",
        "for step in range(10):\n",
        "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
        "    user_input = input(\">> User: \")\n",
        "    if user_input == 'end':\n",
        "      break\n",
        "\n",
        "    new_user_input_ids = tokenizer.encode(user_input + ' ' + tokenizer.eos_token, return_tensors='pt')\n",
        "    print(new_user_input_ids)\n",
        "    history.appendleft(new_user_input_ids)\n",
        "\n",
        "    # append the new user input tokens to the chat history\n",
        "    bot_input_ids = torch.cat(list(history), dim=-1)\n",
        "\n",
        "    # generated a response while limiting the total chat history \n",
        "    chat_history_ids = model.generate(\n",
        "        bot_input_ids, \n",
        "        max_length=2000, \n",
        "        pad_token_id=tokenizer.eos_token_id,  \n",
        "        no_repeat_ngram_size=3,       \n",
        "        do_sample=True, \n",
        "        top_k=100, \n",
        "        top_p=0.9,\n",
        "        temperature = 0.9)\n",
        "    \n",
        "    bot_output_ids = chat_history_ids[:, bot_input_ids.shape[-1]:]\n",
        "    \n",
        "#    print(f'Chat history shape: {chat_history_ids.shape}')\n",
        "    print(f'Chat history: {tokenizer.decode(bot_input_ids[:,:][0], skip_special_tokens=False)}')\n",
        "    \n",
        "    # pretty print last ouput tokens from bot\n",
        "    print(f\"DialoGPT: {tokenizer.decode(bot_output_ids[0], skip_special_tokens=True)}\")\n",
        "    history.append(bot_output_ids)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> User: Hey. How you doing?\n",
            "tensor([[10814,    13,  1374,   345,  1804,    30,   220, 50256]])\n",
            "Chat history: Hey. How you doing? <|endoftext|>\n",
            "DialoGPT: I'll get the popcorn.\n",
            ">> User: Great. Get cokes too. What are we watching?\n",
            "tensor([[13681,    13,  3497,   269,  3369,  1165,    13,  1867,   389,   356,\n",
            "          4964,    30,   220, 50256]])\n",
            "Chat history: Great. Get cokes too. What are we watching? <|endoftext|>Hey. How you doing? <|endoftext|>I'll get the popcorn.<|endoftext|>\n",
            "DialoGPT: So, are you here?\n",
            ">> User: Knocking on the door.\n",
            "tensor([[25095,  8629,   319,   262,  3420,    13,   220, 50256]])\n",
            "Chat history: Knocking on the door. <|endoftext|>Great. Get cokes too. What are we watching? <|endoftext|>Hey. How you doing? <|endoftext|>I'll get the popcorn.<|endoftext|>So, are you here?<|endoftext|>\n",
            "DialoGPT: Just let me get my friend here.\n",
            ">> User: Hurry up. It is starting to rain.\n",
            "tensor([[   39, 16682,   510,    13,   632,   318,  3599,   284,  6290,    13,\n",
            "           220, 50256]])\n",
            "Chat history: Hurry up. It is starting to rain. <|endoftext|>Great. Get cokes too. What are we watching? <|endoftext|>Hey. How you doing? <|endoftext|>I'll get the popcorn.<|endoftext|>So, are you here?<|endoftext|>\n",
            "DialoGPT: I'l get the kool aid\n",
            ">> User: I think you have already drunk it all.\n",
            "tensor([[   40,   892,   345,   423,  1541, 10785,   340,   477,    13,   220,\n",
            "         50256]])\n",
            "Chat history: I think you have already drunk it all. <|endoftext|>Great. Get cokes too. What are we watching? <|endoftext|>Hey. How you doing? <|endoftext|>I'll get the popcorn.<|endoftext|>So, are you here?<|endoftext|>\n",
            "DialoGPT: Yeah!?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLsDlmsgC06f",
        "outputId": "63612b72-42ae-4702-9cb6-25d78afab84c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(tokenizer.eos_token_id)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hh8b_jHtC4nn",
        "outputId": "8f7bfa8d-4465-4295-b17f-b2d12f60b52a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(tokenizer.eos_token)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<|endoftext|>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zA0Zxy7tKZwr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}